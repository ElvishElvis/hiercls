{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a5cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from jax import tree_util\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import hier\n",
    "import infer\n",
    "import main\n",
    "import metrics\n",
    "import progmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf1105",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06047af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configs.inaturalist2021mini\n",
    "\n",
    "base_config = configs.inaturalist2021mini.get_config()\n",
    "base_config.dataset_root = '/home/jack/data/manual/inaturalist2021/'\n",
    "\n",
    "_, eval_dataset, tree, _, eval_label_map = main.make_datasets(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668d8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562d30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_leaf = tree.leaf_mask()\n",
    "specificity = -tree.num_leaf_descendants()\n",
    "not_trivial = (tree.num_children() != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40150867",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_fns = {\n",
    "    'leaf': lambda p: infer.argmax_where(p, is_leaf),\n",
    "    'majority': lambda p: infer.argmax_with_confidence(specificity, p, 0.5, not_trivial),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f0657",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_metric = metrics.UniformLeafInfoMetric(tree)\n",
    "depth_metric = metrics.DepthMetric(tree)\n",
    "\n",
    "metric_fns = {\n",
    "    'exact': lambda gt, pr: pr == gt,\n",
    "    'correct': metrics.IsCorrect(tree),\n",
    "    'info_excess': info_metric.excess,\n",
    "    'info_deficient': info_metric.deficient,\n",
    "    'info_dist': info_metric.dist,\n",
    "    'info_recall': info_metric.recall,\n",
    "    'info_precision': info_metric.precision,\n",
    "    'depth_excess': depth_metric.excess,\n",
    "    'depth_deficient': depth_metric.deficient,\n",
    "    'depth_dist': depth_metric.dist,\n",
    "    'depth_recall': depth_metric.recall,\n",
    "    'depth_precision': depth_metric.precision,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2602504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment = collections.namedtuple('Experiment', ['config_file', 'model_file'])\n",
    "\n",
    "def standard_experiment(experiment_dir, epoch):\n",
    "    return Experiment(\n",
    "        config_file=pathlib.Path(experiment_dir) / 'config.json',\n",
    "        model_file=pathlib.Path(experiment_dir) / f'checkpoints/epoch-{epoch:04d}.pth',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36931a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    'flat': standard_experiment(\n",
    "        experiment_dir='/mnt/ssd1/projects/2022-01-hierarchical/experiments/2022-03-31-inat21mini/flat_softmax-lr-0.01-b-64-wd-0.0003-ep-20/',\n",
    "        epoch=20),\n",
    "    'hier': standard_experiment(\n",
    "        experiment_dir='/mnt/ssd1/projects/2022-01-hierarchical/experiments/2022-03-31-inat21mini/hier_softmax-lr-0.01-b-64-wd-0.0003-ep-20/',\n",
    "        epoch=20),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9e39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {\n",
    "    'leaf': 's',\n",
    "    'majority': 'o',\n",
    "}\n",
    "\n",
    "colors = dict(zip(\n",
    "    experiments,\n",
    "    map(matplotlib.cm.get_cmap('tab10'), itertools.count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321bfc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(net, pred_fn, min_threshold):\n",
    "    # Per-example predictions.\n",
    "\n",
    "    outputs = {\n",
    "        'gt': [],  # Node in hierarchy.\n",
    "        'pred': {method: [] for method in infer_fns},\n",
    "    }\n",
    "    # Sequence-per-example predictions. Cannot be concatenated due to ragged shape.\n",
    "    seq_outputs = {\n",
    "        'pred': [],\n",
    "        'prob': [],\n",
    "    }\n",
    "\n",
    "    net.eval()\n",
    "    with torch.inference_mode():\n",
    "        meter = progmet.ProgressMeter('apply', interval_time=5)\n",
    "        for minibatch in meter(eval_loader):\n",
    "            inputs, gt_labels = minibatch\n",
    "            theta = net(inputs.to(device))\n",
    "            prob = pred_fn(theta).cpu().numpy()\n",
    "            pred = {}\n",
    "            for name, infer_fn in infer_fns.items():\n",
    "                pred[name] = infer_fn(prob)\n",
    "            gt_node = eval_label_map.to_node[gt_labels]\n",
    "            pred_seqs = [\n",
    "                infer.pareto_optimal_predictions(specificity, p, min_threshold, not_trivial)\n",
    "                for p in prob\n",
    "            ]\n",
    "            prob_seqs = [prob[i, pred_i] for i, pred_i in enumerate(pred_seqs)]\n",
    "            # Caution: Predictions are *not* truncated.\n",
    "\n",
    "            outputs['gt'].append(gt_node)\n",
    "            for method in infer_fns:\n",
    "                outputs['pred'][method].append(pred[method])\n",
    "            seq_outputs['pred'].extend(pred_seqs)\n",
    "            seq_outputs['prob'].extend(prob_seqs)\n",
    "\n",
    "    # Concatenate results from minibatches.\n",
    "    leaf_predicate = lambda x: not isinstance(x, dict)  # Treat lists as values, not containers.\n",
    "    outputs = tree_util.tree_map(np.concatenate, outputs, is_leaf=leaf_predicate)\n",
    "\n",
    "    return outputs, seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6303cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_predictions(outputs, seq_outputs):\n",
    "    gt = outputs['gt']\n",
    "    pred = outputs['pred']\n",
    "    pred_seq = seq_outputs['pred']\n",
    "    prob_seq = seq_outputs['prob']\n",
    "\n",
    "    # Evaluate predictions for each method.\n",
    "    pred = {\n",
    "        method: hier.truncate_at_lca(tree, gt, pr)\n",
    "        for method, pr in pred.items()\n",
    "    }\n",
    "    pred_metrics = {\n",
    "        method: {field: np.mean(metric_fn(gt, pr))\n",
    "                 for field, metric_fn in metric_fns.items()}\n",
    "        for method, pr in pred.items()\n",
    "    }\n",
    "\n",
    "    # Evaluate predictions in Pareto sequence.\n",
    "    find_lca = hier.FindLCA(tree)\n",
    "    pred_seq = [hier.truncate_given_lca(gt_i, pr_i, find_lca(gt_i, pr_i)) for gt_i, pr_i in zip(gt, pred_seq)]\n",
    "    metric_values_seq = {\n",
    "        field: [metric_fn(gt_i, pr_i) for gt_i, pr_i in zip(gt, pred_seq)]\n",
    "        for field, metric_fn in metric_fns.items()\n",
    "    }\n",
    "    pareto_scores, pareto_totals = metrics.operating_curve(prob_seq, metric_values_seq)\n",
    "    pareto_means = {k: v / len(gt) for k, v in pareto_totals.items()}\n",
    "\n",
    "    return pred_metrics, pareto_scores, pareto_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, (config_file, model_file) in experiments.items():\n",
    "    if name in results:\n",
    "        print('cached:', name)\n",
    "        continue\n",
    "\n",
    "    # Load model.\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = ml_collections.ConfigDict(json.load(f))\n",
    "    num_outputs = main.get_num_outputs(config.predict, tree)\n",
    "    net = main.make_model(config.model, num_outputs)\n",
    "    missing_keys, unexpected_keys = net.load_state_dict(torch.load(model_file), strict=True)\n",
    "    assert not missing_keys\n",
    "    assert not unexpected_keys\n",
    "\n",
    "    net.to(device)\n",
    "    _, pred_fn = main.make_loss(config, tree, device)\n",
    "    outputs, seq_outputs = apply_model(net, pred_fn, min_threshold=0.1)\n",
    "    pred_metrics, pareto_scores, pareto_metrics = assess_predictions(outputs, seq_outputs)\n",
    "    results[name] = {\n",
    "        'pred_metrics': pred_metrics,\n",
    "        'pareto_scores': pareto_scores,\n",
    "        'pareto_metrics': pareto_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19078a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(x, y):\n",
    "    for name in results:\n",
    "        pred_metrics = results[name]['pred_metrics']\n",
    "        pareto_scores = results[name]['pareto_scores']\n",
    "        pareto_metrics = results[name]['pareto_metrics']\n",
    "        ge = np.concatenate(([True], pareto_scores >= 0.5))\n",
    "        le = np.concatenate(([False], pareto_scores <= 0.5))\n",
    "        plt.plot(pareto_metrics[x][ge], pareto_metrics[y][ge],\n",
    "                 color=colors[name], label=name)\n",
    "        plt.plot(pareto_metrics[x][le], pareto_metrics[y][le],\n",
    "                 color=colors[name], linestyle='--')\n",
    "        for method, method_metrics in pred_metrics.items():\n",
    "            plt.plot(method_metrics[x], method_metrics[y], color=colors[name],\n",
    "                     marker=markers[method], markerfacecolor='none')\n",
    "    plt.ylim(top=1)\n",
    "    plt.xlim(left=0)\n",
    "    # plt.axis('equal')\n",
    "    # plt.gca().set_aspect(1)\n",
    "    plt.grid()\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e6cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('exact', 'correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de93848",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('depth_recall', 'correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('depth_recall', 'depth_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbf2628",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('info_recall', 'info_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c3302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
