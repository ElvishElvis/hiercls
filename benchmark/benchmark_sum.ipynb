{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2886217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86745e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# TODO: pip install -e hier\n",
    "PROJECT_ROOT = pathlib.Path('.').absolute().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "import hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2ca3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_DIR = pathlib.Path('../resources')\n",
    "with open(RESOURCE_DIR / 'hierarchy/imagenet_fiveai.csv') as f:\n",
    "    edges = hier.load_edges(f)\n",
    "tree, names = hier.make_hierarchy_from_edges(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f080bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1372)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.num_leaf_nodes(), tree.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2929e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1024\n",
    "d = tree.num_leaf_nodes()\n",
    "device = torch.device('cuda')\n",
    "\n",
    "theta = torch.randn([b, d]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_leaf_descendants(\n",
    "        tree: hier.Hierarchy,\n",
    "        values: torch.Tensor,\n",
    "        dim: int = -1) -> torch.Tensor:\n",
    "    \"\"\"Computes sum over leaf descendants for each node.\"\"\"\n",
    "    # The value is_ancestor[i, j] is true if i is an ancestor of j.\n",
    "    is_ancestor = tree.ancestor_mask()\n",
    "    leaf_is_descendant = is_ancestor[:, tree.leaf_mask()].T\n",
    "    matrix = torch.from_numpy(leaf_is_descendant)\n",
    "    matrix = matrix.to(device=values.device, dtype=torch.get_default_dtype())\n",
    "    # TODO: Re-order dimensions to make this work with dim != -1.\n",
    "    assert dim == -1 or dim == values.ndim - 1\n",
    "    return torch.tensordot(values, matrix, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095599b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(nn.Module):\n",
    "    \"\"\"Avoids re-computation in sum_xxx().\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tree: hier.Hierarchy,\n",
    "            leaf: bool,\n",
    "            transpose: bool,\n",
    "            strict: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # The value is_ancestor[i, j] is true if i is an ancestor of j.\n",
    "        matrix = tree.ancestor_mask(strict=strict)\n",
    "        if leaf:\n",
    "            matrix = matrix[:, tree.leaf_mask()]\n",
    "        if transpose:\n",
    "            matrix = matrix.T\n",
    "        matrix = torch.from_numpy(matrix).type(torch.get_default_dtype())\n",
    "\n",
    "        # self.matrix = matrix\n",
    "        self.register_buffer('matrix', matrix)\n",
    "        self.matrix: Optional[torch.Tensor]\n",
    "\n",
    "    def forward(self, values: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "        # TODO: Re-order dimensions to make this work with dim != -1.\n",
    "        assert dim == -1 or dim == values.ndim - 1\n",
    "        return torch.tensordot(values, self.matrix, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "SumAncestors = partial(Sum, leaf=False, transpose=False)\n",
    "SumLeafAncestors = partial(Sum, leaf=False, transpose=True)\n",
    "SumDescendants = partial(Sum, leaf=False, transpose=True)\n",
    "SumLeafDescendants = partial(Sum, leaf=True, transpose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832efe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sum_leaf_descendants_sparse(\n",
    "#         tree: hier.Hierarchy,\n",
    "#         values: torch.Tensor,\n",
    "#         dim: int = -1) -> torch.Tensor:\n",
    "#     \"\"\"Computes sum over leaf descendants for each node.\"\"\"\n",
    "#     # The value is_ancestor[i, j] is true if i is an ancestor of j.\n",
    "#     is_ancestor = tree.ancestor_mask()\n",
    "#     leaf_is_descendant = is_ancestor[:, tree.leaf_mask()].T\n",
    "#     matrix = torch.from_numpy(leaf_is_descendant)\n",
    "#     matrix = matrix.to(device=values.device, dtype=torch.get_default_dtype())\n",
    "#     # TODO: Re-order dimensions to make this work with dim != -1.\n",
    "#     assert dim == -1 or dim == values.ndim - 1\n",
    "#     return torch.tensordot(values, matrix, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dense = benchmark.Timer(\n",
    "    stmt='sum_leaf_descendants(tree, theta)',\n",
    "    setup='from __main__ import sum_leaf_descendants',\n",
    "    globals={\n",
    "        'tree': tree,\n",
    "        'theta': theta,\n",
    "    })\n",
    "\n",
    "t_dense_pre = benchmark.Timer(\n",
    "    stmt='sum_fn(theta)',\n",
    "    globals={\n",
    "        'sum_fn': SumLeafDescendants(tree).to(device),\n",
    "        'theta': theta,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6528c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dense.blocked_autorange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dense_pre.blocked_autorange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73df6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tree.ancestor_mt_dense.blocked_autorange()ask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9236f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_subset = [np.flatnonzero(row) for row in matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a47a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ee6124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01170c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1024\n",
    "d = tree.num_nodes() - 1\n",
    "device = torch.device('cuda')\n",
    "\n",
    "theta = torch.randn([b, d]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ea8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hier_log_softmax(\n",
    "        tree: hier.Hierarchy,\n",
    "        scores: torch.Tensor,\n",
    "        dim: int = -1) -> torch.Tensor:\n",
    "    internal_nodes = tree.internal_subset()\n",
    "    node_to_children = tree.children()\n",
    "    cond_children = [node_to_children[x] for x in internal_nodes]\n",
    "    cond_sizes = [len(x) for x in cond_children]\n",
    "    cond_scores = scores.split(cond_sizes, dim=dim)\n",
    "    cond_log_softmax = [x.log_softmax(dim=dim) for x in cond_scores]\n",
    "    shape = list(scores.shape)\n",
    "    shape[dim] = tree.num_nodes()\n",
    "    log_cond_prob = torch.zeros(shape, device=scores.device).index_add(\n",
    "        dim,\n",
    "        torch.from_numpy(np.concatenate(cond_children)).to(scores.device),\n",
    "        torch.cat(cond_log_softmax, dim=dim))\n",
    "    log_prob = sum_ancestors(tree, log_cond_prob, dim=dim, strict=False)\n",
    "    return log_prob\n",
    "\n",
    "\n",
    "def sum_ancestors(\n",
    "        tree: hier.Hierarchy,\n",
    "        values: torch.Tensor,\n",
    "        dim: int = -1,\n",
    "        strict: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Computes sum over ancestors of each node.\"\"\"\n",
    "    # The value is_ancestor[i, j] is true if i is an ancestor of j.\n",
    "    is_ancestor = tree.ancestor_mask(strict=strict)\n",
    "    matrix = (torch.from_numpy(is_ancestor)\n",
    "              .to(device=values.device, dtype=torch.get_default_dtype()))\n",
    "    # TODO: Re-order dimensions to make this work with dim != -1.\n",
    "    assert dim == -1 or dim == values.ndim - 1\n",
    "    return torch.tensordot(values, matrix, dims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b131449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierLogSoftmax(nn.Module):\n",
    "    \"\"\"Avoids re-computation in hier_log_softmax().\"\"\"\n",
    "\n",
    "    def __init__(self, tree: hier.Hierarchy):\n",
    "        super().__init__()\n",
    "        internal_nodes = tree.internal_subset()\n",
    "        node_to_children = tree.children()\n",
    "        cond_children = [node_to_children[x] for x in internal_nodes]\n",
    "        cond_sizes = [len(x) for x in cond_children]\n",
    "        cat_cond_children = torch.from_numpy(np.concatenate(cond_children))\n",
    "\n",
    "        self.cond_sizes = cond_sizes\n",
    "        self.num_nodes = tree.num_nodes()\n",
    "        # self.cat_cond_children = cat_cond_children\n",
    "        self.register_buffer('cat_cond_children', cat_cond_children)\n",
    "        self.cat_cond_children: Optional[torch.Tensor]\n",
    "        self.sum_ancestors = SumAncestors(tree, strict=False)\n",
    "\n",
    "    def forward(self, scores: torch.Tensor, dim: int = -1) -> torch.Tensor:\n",
    "        device = scores.device\n",
    "        cond_scores = scores.split(self.cond_sizes, dim=dim)\n",
    "        cond_log_softmax = [x.log_softmax(dim=dim) for x in cond_scores]\n",
    "        shape = list(scores.shape)\n",
    "        shape[dim] = self.num_nodes\n",
    "        log_cond_prob = torch.zeros(shape, device=device).index_add(\n",
    "            dim, self.cat_cond_children, torch.cat(cond_log_softmax, dim=dim))\n",
    "        log_prob = self.sum_ancestors(log_cond_prob, dim=dim)\n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb768142",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_baseline = benchmark.Timer(\n",
    "    stmt='theta.log_softmax(dim=-1)',\n",
    "    globals={\n",
    "        'theta': theta,\n",
    "    })\n",
    "\n",
    "t_dense = benchmark.Timer(\n",
    "    stmt='hier_log_softmax(tree, theta)',\n",
    "    setup='from __main__ import hier_log_softmax',\n",
    "    globals={\n",
    "        'tree': tree,\n",
    "        'theta': theta,\n",
    "    })\n",
    "\n",
    "t_dense_pre = benchmark.Timer(\n",
    "    stmt='hier_log_softmax_fn(theta)',\n",
    "    globals={\n",
    "        'hier_log_softmax_fn': HierLogSoftmax(tree).to(device),\n",
    "        'theta': theta,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_baseline.blocked_autorange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89586546",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dense.blocked_autorange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dense_pre.blocked_autorange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42877ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
