{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a40757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "from jax import tree_util\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import ml_collections\n",
    "import numpy as np\n",
    "import pandas\n",
    "import torch\n",
    "\n",
    "import hier\n",
    "import infer\n",
    "import main\n",
    "import metrics\n",
    "import progmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09a9551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77e98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configs.tiny_imagenet\n",
    "\n",
    "base_config = configs.tiny_imagenet.get_config()\n",
    "base_config.dataset_root = '/home/jack/data/manual/tiny_imagenet/'\n",
    "\n",
    "_, eval_dataset, tree, node_names, _, eval_label_map = main.make_datasets(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1a550d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    dataset=eval_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae01bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While metrics will be computed using the full tree, we will project\n",
    "# the predictions to a subset of the tree before evaluating.\n",
    "\n",
    "# Expect that \"exact\" will be zero but \"correct\" will be higher than usual.\n",
    "\n",
    "def load_subtree(fname, tree, node_names):\n",
    "    # We will evaluate the model using a sub-tree defined by a class subset.\n",
    "    name_to_ind = {x: i for i, x in enumerate(node_names)}\n",
    "    with open(fname) as f:\n",
    "        min_subset_names = set([line.strip() for line in f])\n",
    "    min_subset_inds = set([name_to_ind[name] for name in min_subset_names])\n",
    "    return hier.subtree(tree, min_subset_inds)\n",
    "\n",
    "subset_fname = 'resources/class_subset/tiny_imagenet_fiveai_max_leaf_size_20.txt'\n",
    "subtree, node_subset, project_to_subtree = load_subtree(subset_fname, tree, node_names)\n",
    "subtree_node_names = [node_names[i] for i in node_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a52218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment = collections.namedtuple('Experiment', ['config_file', 'model_file'])\n",
    "\n",
    "def standard_experiment(experiment_dir, epoch):\n",
    "    return Experiment(\n",
    "        config_file=pathlib.Path(experiment_dir) / 'config.json',\n",
    "        model_file=pathlib.Path(experiment_dir) / f'checkpoints/epoch-{epoch:04d}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfcf4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    'full': standard_experiment(\n",
    "        experiment_dir='/mnt/ssd1/projects/2022-01-hierarchical/experiments/2022-04-04-tiny-imagenet-cut/flat/',\n",
    "        epoch=100),\n",
    "    'min-leaf-5': standard_experiment(\n",
    "        experiment_dir='/mnt/ssd1/projects/2022-01-hierarchical/experiments/2022-04-04-tiny-imagenet-cut/flat-max-leaf-size-20/',\n",
    "        epoch=100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7b37280",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = {\n",
    "    'leaf': 's',\n",
    "    'majority': 'o',\n",
    "}\n",
    "\n",
    "colors = dict(zip(\n",
    "    experiments,\n",
    "    map(matplotlib.cm.get_cmap('tab10'), itertools.count())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f3b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference in the sub-tree.\n",
    "\n",
    "is_leaf = subtree.leaf_mask()\n",
    "specificity = -subtree.num_leaf_descendants()\n",
    "not_trivial = (subtree.num_children() != 1)\n",
    "\n",
    "infer_fns = {\n",
    "    'leaf': lambda p: infer.argmax_where(p, is_leaf),\n",
    "    'majority': lambda p: infer.argmax_with_confidence(specificity, p, 0.5, not_trivial),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5658509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(net, pred_fn, prob_subset, min_threshold):\n",
    "    # Per-example predictions.\n",
    "\n",
    "    outputs = {\n",
    "        'gt': [],  # Node in hierarchy.\n",
    "        'pred': {method: [] for method in infer_fns},\n",
    "    }\n",
    "    # Sequence-per-example predictions. Cannot be concatenated due to ragged shape.\n",
    "    seq_outputs = {\n",
    "        'pred': [],\n",
    "        'prob': [],\n",
    "    }\n",
    "\n",
    "    net.eval()\n",
    "    with torch.inference_mode():\n",
    "        meter = progmet.ProgressMeter('apply', interval_time=5)\n",
    "        for minibatch in meter(eval_loader):\n",
    "            inputs, gt_labels = minibatch\n",
    "            gt_node = eval_label_map.to_node[gt_labels]\n",
    "            theta = net(inputs.to(device))\n",
    "            prob = pred_fn(theta).cpu().numpy()\n",
    "\n",
    "            # Perform inference in sub-tree.\n",
    "            # Take subset of probability vector if necessary.\n",
    "            prob = prob[..., prob_subset]\n",
    "\n",
    "            pred = {}\n",
    "            for method, infer_fn in infer_fns.items():\n",
    "                pred[method] = infer_fn(prob)\n",
    "            pred_seqs = [\n",
    "                infer.pareto_optimal_predictions(specificity, p, min_threshold, not_trivial)\n",
    "                for p in prob\n",
    "            ]\n",
    "            prob_seqs = [prob[i, pred_i] for i, pred_i in enumerate(pred_seqs)]\n",
    "\n",
    "            # Correctness should not change whether evaluated in full tree or sub-tree.\n",
    "            # (This may be violated if leaf nodes do not map to leaf nodes in the sub-tree.)\n",
    "            full_gt = gt_node\n",
    "            sub_gt = project_to_subtree[full_gt]\n",
    "            sub_pred = pred['majority']\n",
    "            full_pred = node_subset[sub_pred]\n",
    "            assert np.all(\n",
    "                metrics.correct(tree, full_gt, full_pred) ==\n",
    "                metrics.correct(subtree, sub_gt, sub_pred))\n",
    "\n",
    "            # Evaluate metrics in full tree.\n",
    "            # Apply inverse projection to return to larger label space.\n",
    "            pred = {method: node_subset[pred[method]] for method in pred}\n",
    "            pred_seqs = [node_subset[seq] for seq in pred_seqs]\n",
    "\n",
    "            # The alternative would be to return gt and pred in the sub-tree.\n",
    "            # To do this, we must project the ground-truth.\n",
    "            # gt_node = project_to_subtree[gt_node]\n",
    "\n",
    "            outputs['gt'].append(gt_node)\n",
    "            for method in infer_fns:\n",
    "                outputs['pred'][method].append(pred[method])\n",
    "            seq_outputs['pred'].extend(pred_seqs)\n",
    "            seq_outputs['prob'].extend(prob_seqs)\n",
    "\n",
    "    # Concatenate results from minibatches.\n",
    "    leaf_predicate = lambda x: not isinstance(x, dict)  # Treat lists as values, not containers.\n",
    "    outputs = tree_util.tree_map(np.concatenate, outputs, is_leaf=leaf_predicate)\n",
    "\n",
    "    return outputs, seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bf6d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_predictions(tree, outputs, seq_outputs):\n",
    "    gt = outputs['gt']\n",
    "    pred = outputs['pred']\n",
    "    pred_seq = seq_outputs['pred']\n",
    "    prob_seq = seq_outputs['prob']\n",
    "\n",
    "    info_metric = metrics.UniformLeafInfoMetric(tree)\n",
    "    depth_metric = metrics.DepthMetric(tree)\n",
    "    metric_fns = {\n",
    "        'exact': lambda gt, pr: pr == gt,\n",
    "        'correct': metrics.IsCorrect(tree),\n",
    "        'info_excess': info_metric.excess,\n",
    "        'info_deficient': info_metric.deficient,\n",
    "        'info_dist': info_metric.dist,\n",
    "        'info_recall': info_metric.recall,\n",
    "        'info_precision': info_metric.precision,\n",
    "        'depth_excess': depth_metric.excess,\n",
    "        'depth_deficient': depth_metric.deficient,\n",
    "        'depth_dist': depth_metric.dist,\n",
    "        'depth_recall': depth_metric.recall,\n",
    "        'depth_precision': depth_metric.precision,\n",
    "    }\n",
    "\n",
    "    # Evaluate predictions for each method.\n",
    "    pred = {\n",
    "        method: hier.truncate_at_lca(tree, gt, pr)\n",
    "        for method, pr in pred.items()\n",
    "    }\n",
    "    pred_metrics = {\n",
    "        method: {field: np.mean(metric_fn(gt, pr))\n",
    "                 for field, metric_fn in metric_fns.items()}\n",
    "        for method, pr in pred.items()\n",
    "    }\n",
    "\n",
    "    # Evaluate predictions in Pareto sequence.\n",
    "    find_lca = hier.FindLCA(tree)\n",
    "    pred_seq = [hier.truncate_given_lca(gt_i, pr_i, find_lca(gt_i, pr_i)) for gt_i, pr_i in zip(gt, pred_seq)]\n",
    "    metric_values_seq = {\n",
    "        field: [metric_fn(gt_i, pr_i) for gt_i, pr_i in zip(gt, pred_seq)]\n",
    "        for field, metric_fn in metric_fns.items()\n",
    "    }\n",
    "    pareto_scores, pareto_totals = metrics.operating_curve(prob_seq, metric_values_seq)\n",
    "    pareto_means = {k: v / len(gt) for k, v in pareto_totals.items()}\n",
    "\n",
    "    return pred_metrics, pareto_scores, pareto_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c14606ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4ae2ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "apply: 100% (40/40); T=0.0167 f=59.9; mean T=0.0167 f=59.9; elapsed 0:00:01\n",
      "apply: 100% (40/40); T=0.0151 f=66.0; mean T=0.0151 f=66.0; elapsed 0:00:01\n"
     ]
    }
   ],
   "source": [
    "for name, (config_file, model_file) in experiments.items():\n",
    "    if name in results:\n",
    "        print('cached:', name)\n",
    "        continue\n",
    "\n",
    "    # Load model.\n",
    "    with open(config_file, 'r') as f:\n",
    "        config = ml_collections.ConfigDict(json.load(f))\n",
    "\n",
    "    # Depending on which tree was used during training,\n",
    "    # may need to extract subset of probability vector.\n",
    "    _, _, train_tree, train_node_names, _, _ = main.make_datasets(config)\n",
    "    # Find location of the subtree nodes in the train nodes.\n",
    "    # These will be the indices that we extract from the probability vector.\n",
    "    prob_subset = [train_node_names.index(node) for node in subtree_node_names]\n",
    "\n",
    "    num_outputs = main.get_num_outputs(config.predict, train_tree)\n",
    "    net = main.make_model(config.model, num_outputs)\n",
    "    net.load_state_dict(torch.load(model_file), strict=True)\n",
    "\n",
    "    net.to(device)\n",
    "    _, pred_fn = main.make_loss(config, train_tree, device)\n",
    "    outputs, seq_outputs = apply_model(net, pred_fn, prob_subset, min_threshold=0.1)\n",
    "    pred_metrics, pareto_scores, pareto_metrics = assess_predictions(tree, outputs, seq_outputs)\n",
    "    # pred_metrics, pareto_scores, pareto_metrics = assess_predictions(subtree, outputs, seq_outputs)\n",
    "\n",
    "    results[name] = {\n",
    "        'pred_metrics': pred_metrics,\n",
    "        'pareto_scores': pareto_scores,\n",
    "        'pareto_metrics': pareto_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a1e3d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>exact</th>\n",
       "      <th>correct</th>\n",
       "      <th>info_excess</th>\n",
       "      <th>info_deficient</th>\n",
       "      <th>info_dist</th>\n",
       "      <th>info_recall</th>\n",
       "      <th>info_precision</th>\n",
       "      <th>depth_excess</th>\n",
       "      <th>depth_deficient</th>\n",
       "      <th>depth_dist</th>\n",
       "      <th>depth_recall</th>\n",
       "      <th>depth_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">full</th>\n",
       "      <th>leaf</th>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.6134</td>\n",
       "      <td>1.456932</td>\n",
       "      <td>4.527882</td>\n",
       "      <td>5.984814</td>\n",
       "      <td>0.407644</td>\n",
       "      <td>0.689520</td>\n",
       "      <td>0.8270</td>\n",
       "      <td>3.8067</td>\n",
       "      <td>4.6337</td>\n",
       "      <td>0.520514</td>\n",
       "      <td>0.829561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>majority</th>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.7341</td>\n",
       "      <td>0.765205</td>\n",
       "      <td>4.778567</td>\n",
       "      <td>5.543772</td>\n",
       "      <td>0.374849</td>\n",
       "      <td>0.792605</td>\n",
       "      <td>0.5305</td>\n",
       "      <td>3.8982</td>\n",
       "      <td>4.4287</td>\n",
       "      <td>0.509378</td>\n",
       "      <td>0.878995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">min-leaf-5</th>\n",
       "      <th>leaf</th>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.5938</td>\n",
       "      <td>1.505366</td>\n",
       "      <td>4.629045</td>\n",
       "      <td>6.134411</td>\n",
       "      <td>0.394410</td>\n",
       "      <td>0.673787</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>3.8509</td>\n",
       "      <td>4.7285</td>\n",
       "      <td>0.514342</td>\n",
       "      <td>0.818051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>majority</th>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.6470</td>\n",
       "      <td>1.197004</td>\n",
       "      <td>4.700515</td>\n",
       "      <td>5.897520</td>\n",
       "      <td>0.385060</td>\n",
       "      <td>0.718975</td>\n",
       "      <td>0.7461</td>\n",
       "      <td>3.8685</td>\n",
       "      <td>4.6146</td>\n",
       "      <td>0.512079</td>\n",
       "      <td>0.840903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      exact  correct  info_excess  info_deficient  info_dist  \\\n",
       "full       leaf      0.0108   0.6134     1.456932        4.527882   5.984814   \n",
       "           majority  0.0090   0.7341     0.765205        4.778567   5.543772   \n",
       "min-leaf-5 leaf      0.0094   0.5938     1.505366        4.629045   6.134411   \n",
       "           majority  0.0087   0.6470     1.197004        4.700515   5.897520   \n",
       "\n",
       "                     info_recall  info_precision  depth_excess  \\\n",
       "full       leaf         0.407644        0.689520        0.8270   \n",
       "           majority     0.374849        0.792605        0.5305   \n",
       "min-leaf-5 leaf         0.394410        0.673787        0.8776   \n",
       "           majority     0.385060        0.718975        0.7461   \n",
       "\n",
       "                     depth_deficient  depth_dist  depth_recall  \\\n",
       "full       leaf               3.8067      4.6337      0.520514   \n",
       "           majority           3.8982      4.4287      0.509378   \n",
       "min-leaf-5 leaf               3.8509      4.7285      0.514342   \n",
       "           majority           3.8685      4.6146      0.512079   \n",
       "\n",
       "                     depth_precision  \n",
       "full       leaf             0.829561  \n",
       "           majority         0.878995  \n",
       "min-leaf-5 leaf             0.818051  \n",
       "           majority         0.840903  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas.DataFrame.from_dict({\n",
    "    (name, method): results[name]['pred_metrics'][method]\n",
    "    for name in results\n",
    "    for method in results[name]['pred_metrics']\n",
    "}, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacdba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(x, y):\n",
    "    for name in results:\n",
    "        pred_metrics = results[name]['pred_metrics']\n",
    "        pareto_scores = results[name]['pareto_scores']\n",
    "        pareto_metrics = results[name]['pareto_metrics']\n",
    "        ge = np.concatenate(([True], pareto_scores >= 0.5))\n",
    "        le = np.concatenate(([False], pareto_scores <= 0.5))\n",
    "        plt.plot(pareto_metrics[x][ge], pareto_metrics[y][ge],\n",
    "                 color=colors[name], label=name)\n",
    "        plt.plot(pareto_metrics[x][le], pareto_metrics[y][le],\n",
    "                 color=colors[name], linestyle='--')\n",
    "        for method, method_metrics in pred_metrics.items():\n",
    "            plt.plot(method_metrics[x], method_metrics[y], color=colors[name],\n",
    "                     marker=markers[method], markerfacecolor='none')\n",
    "    plt.ylim(top=1)\n",
    "    plt.xlim(left=0)\n",
    "    # plt.axis('equal')\n",
    "    # plt.gca().set_aspect(1)\n",
    "    plt.grid()\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2893e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('depth_recall', 'correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f6855",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('depth_recall', 'depth_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47951bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics('info_recall', 'info_precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610fa1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
